{
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "sourceId": 11267456,
     "sourceType": "datasetVersion",
     "datasetId": 7043178
    }
   ],
   "dockerImageVersionId": 30919,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 0. Imports and requirements"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import torch\n",
    "from tqdm import trange\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tqdm \n",
    "\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "sys.path.append('/kaggle/input/credit') # замени на свою\n",
    "sys.path.append('../')\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:46:57.217773Z",
     "iopub.execute_input": "2025-04-18T11:46:57.218042Z",
     "iopub.status.idle": "2025-04-18T11:47:02.656349Z",
     "shell.execute_reply.started": "2025-04-18T11:46:57.218010Z",
     "shell.execute_reply": "2025-04-18T11:47:02.655628Z"
    }
   },
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:13.609353Z",
     "iopub.execute_input": "2025-04-18T11:47:13.609680Z",
     "iopub.status.idle": "2025-04-18T11:47:13.636612Z",
     "shell.execute_reply.started": "2025-04-18T11:47:13.609653Z",
     "shell.execute_reply": "2025-04-18T11:47:13.635826Z"
    }
   },
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Data Preprocessing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import dataset_preprocessing_utils\n",
    "print(dir(dataset_preprocessing_utils))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:15.179257Z",
     "iopub.execute_input": "2025-04-18T11:47:15.179544Z",
     "iopub.status.idle": "2025-04-18T11:47:15.338454Z",
     "shell.execute_reply.started": "2025-04-18T11:47:15.179523Z",
     "shell.execute_reply": "2025-04-18T11:47:15.337763Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['Dict', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'create_padded_buckets', 'features', 'np', 'pad_sequence', 'pd', 'pickle', 'tqdm', 'transform_credits_to_sequences', 'truncate']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": [
    "# тут тоже поставь свое \n",
    "TRAIN_DATA_PATH = \"/kaggle/input/credit/data/train_data/\" \n",
    "TEST_DATA_PATH = \"/kaggle/input/credit/data/test_data/\"\n",
    "\n",
    "TRAIN_TARGET_PATH = \"/kaggle/input/credit/data/train_target.csv\""
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:17.707102Z",
     "iopub.execute_input": "2025-04-18T11:47:17.707389Z",
     "iopub.status.idle": "2025-04-18T11:47:17.734034Z",
     "shell.execute_reply.started": "2025-04-18T11:47:17.707358Z",
     "shell.execute_reply": "2025-04-18T11:47:17.733157Z"
    }
   },
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "train_target = pd.read_csv(TRAIN_TARGET_PATH)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:21.988112Z",
     "iopub.execute_input": "2025-04-18T11:47:21.988393Z",
     "iopub.status.idle": "2025-04-18T11:47:22.639448Z",
     "shell.execute_reply.started": "2025-04-18T11:47:21.988364Z",
     "shell.execute_reply": "2025-04-18T11:47:22.638516Z"
    }
   },
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import read_parquet_dataset_from_local\n",
    "from dataset_preprocessing_utils import features, transform_credits_to_sequences, create_padded_buckets"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:23.099607Z",
     "iopub.execute_input": "2025-04-18T11:47:23.100190Z",
     "iopub.status.idle": "2025-04-18T11:47:23.129183Z",
     "shell.execute_reply.started": "2025-04-18T11:47:23.100161Z",
     "shell.execute_reply": "2025-04-18T11:47:23.128189Z"
    }
   },
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "train_lens = []\n",
    "test_lens = []\n",
    "uniques = defaultdict(set)\n",
    "\n",
    "for step in tqdm.notebook.tqdm(range(0, 12, 4),\n",
    "                     desc=\"Count statistics on train data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(TRAIN_DATA_PATH, step, 4, verbose=True)\n",
    "        seq_lens = credits_frame.groupby(\"id\").agg(seq_len=(\"rn\", \"max\"))[\"seq_len\"].values\n",
    "        train_lens.extend(seq_lens)\n",
    "        credits_frame.drop(columns=[\"id\", \"rn\"], inplace=True)\n",
    "        for feat in credits_frame.columns.values:\n",
    "            uniques[feat] = uniques[feat].union(credits_frame[feat].unique())\n",
    "train_lens = np.hstack(train_lens)\n",
    "\n",
    "for step in tqdm.notebook.tqdm(range(0, 2, 2),\n",
    "                     desc=\"Count statistics on test data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(TEST_DATA_PATH, step, 2, verbose=True)\n",
    "        seq_lens = credits_frame.groupby(\"id\").agg(seq_len=(\"rn\", \"max\"))[\"seq_len\"].values\n",
    "        test_lens.extend(seq_lens)\n",
    "        credits_frame.drop(columns=[\"id\", \"rn\"], inplace=True)\n",
    "        for feat in credits_frame.columns.values:\n",
    "            uniques[feat] = uniques[feat].union(credits_frame[feat].unique())\n",
    "test_lens = np.hstack(test_lens)\n",
    "uniques = dict(uniques)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:22:43.379214Z",
     "iopub.execute_input": "2025-04-18T12:22:43.379546Z",
     "iopub.status.idle": "2025-04-18T12:23:30.899569Z",
     "shell.execute_reply.started": "2025-04-18T12:22:43.379521Z",
     "shell.execute_reply": "2025-04-18T12:23:30.898679Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Count statistics on train data:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "989b9927e0944512a57b234721b116aa"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_0.pq\n/kaggle/input/credit/data/train_data/train_data_1.pq\n/kaggle/input/credit/data/train_data/train_data_2.pq\n/kaggle/input/credit/data/train_data/train_data_3.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "00f1df53195647d7b79ea948c0679893"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_4.pq\n/kaggle/input/credit/data/train_data/train_data_5.pq\n/kaggle/input/credit/data/train_data/train_data_6.pq\n/kaggle/input/credit/data/train_data/train_data_7.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e2e98da737f741a8ad77a6078d7c7bc4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_8.pq\n/kaggle/input/credit/data/train_data/train_data_9.pq\n/kaggle/input/credit/data/train_data/train_data_10.pq\n/kaggle/input/credit/data/train_data/train_data_11.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "20d6063d1457438ba7e41339eef696cc"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Count statistics on test data:   0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9b8673151d054bd49ca14a993be2148b"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/test_data/test_data_0.pq\n/kaggle/input/credit/data/test_data/test_data_1.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "255cde8c52db481884a525a27f5fa400"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "CPU times: user 46.8 s, sys: 34.4 s, total: 1min 21s\nWall time: 47.5 s\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "source": [
    "* В дальнейшем при построении рекуррентной нейронной сети нам понадобятся следующие статистики по тренировочной и тестовой выборкам: распределение длин кредитных историй и число уникальных значений каждого категориального значения. Посчитаем эти статистики:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Чтобы сразу убедиться, что посчитанные статистики интересные и полезные, построим графики распределений длин кредитных историй в тренировочной и тестовой выборках"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import dataset_preprocessing_utils\n",
    "print(dir(dataset_preprocessing_utils))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:26.766799Z",
     "iopub.execute_input": "2025-04-18T11:47:26.767147Z",
     "iopub.status.idle": "2025-04-18T11:47:26.794650Z",
     "shell.execute_reply.started": "2025-04-18T11:47:26.767119Z",
     "shell.execute_reply": "2025-04-18T11:47:26.793986Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "['Dict', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'create_padded_buckets', 'features', 'np', 'pad_sequence', 'pd', 'pickle', 'tqdm', 'transform_credits_to_sequences', 'truncate']\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Один из аргументов в функции `dataset_preprocessing_utils.create_padded_buckets` &ndash; `bucket_info` &ndash; словарь, где для конкретной длины последовательности указано до какой длины нужно делать паддинг. Для данного бэйзлайна возьмем простое разбиение на 43 бакета: \n",
    "| Длина последовательности | Длина после паддинга |\n",
    "| :-: | :-: \n",
    "| 1 &ndash; 40 | без изменений |\n",
    "| 41 &ndash; 45 | 45 |\n",
    "| 46 &ndash; 50 | 50 |\n",
    "| 51 &ndash; 58 | 58 |"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "keys_ = list(range(1, 59)) \n",
    "lens_ = list(range(1, 41)) + [45] * 5 + [50] * 5 + [58] * 8\n",
    "bucket_info = dict(zip(keys_, lens_))\n",
    "bucket_info"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:30.449998Z",
     "iopub.execute_input": "2025-04-18T11:47:30.450309Z",
     "iopub.status.idle": "2025-04-18T11:47:30.483473Z",
     "shell.execute_reply.started": "2025-04-18T11:47:30.450282Z",
     "shell.execute_reply": "2025-04-18T11:47:30.482249Z"
    }
   },
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{1: 1,\n 2: 2,\n 3: 3,\n 4: 4,\n 5: 5,\n 6: 6,\n 7: 7,\n 8: 8,\n 9: 9,\n 10: 10,\n 11: 11,\n 12: 12,\n 13: 13,\n 14: 14,\n 15: 15,\n 16: 16,\n 17: 17,\n 18: 18,\n 19: 19,\n 20: 20,\n 21: 21,\n 22: 22,\n 23: 23,\n 24: 24,\n 25: 25,\n 26: 26,\n 27: 27,\n 28: 28,\n 29: 29,\n 30: 30,\n 31: 31,\n 32: 32,\n 33: 33,\n 34: 34,\n 35: 35,\n 36: 36,\n 37: 37,\n 38: 38,\n 39: 39,\n 40: 40,\n 41: 45,\n 42: 45,\n 43: 45,\n 44: 45,\n 45: 45,\n 46: 50,\n 47: 50,\n 48: 50,\n 49: 50,\n 50: 50,\n 51: 58,\n 52: 58,\n 53: 58,\n 54: 58,\n 55: 58,\n 56: 58,\n 57: 58,\n 58: 58}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Так же рассмотрим уникальные значения признаков"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Поскольку паддинг будет производиться нулями, а категориальные признаки закодированы, начиная с 0, перед паддингом будем сдвигать все значения на 1."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Вся описанная выше предобработка данных реализована в виде функции `create_buckets_from_credits`:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def create_buckets_from_credits(path_to_dataset, bucket_info, save_to_path, frame_with_ids = None, \n",
    "                                num_parts_to_preprocess_at_once: int = 1, \n",
    "                                num_parts_total=50, has_target=False):\n",
    "    block = 0\n",
    "    for step in tqdm.notebook.tqdm(range(0, num_parts_total, num_parts_to_preprocess_at_once),\n",
    "                     desc=\"Preparing credit data\"):\n",
    "        credits_frame = read_parquet_dataset_from_local(path_to_dataset, step, num_parts_to_preprocess_at_once, verbose=True)\n",
    "        credits_frame.loc[:, features] += 1       \n",
    "        seq = transform_credits_to_sequences(credits_frame)\n",
    "        print(\"Transforming credits to sequences is done.\")\n",
    "        \n",
    "        if frame_with_ids is not None:\n",
    "            seq = seq.merge(frame_with_ids, on=\"id\")\n",
    "\n",
    "        block_as_str = str(block)\n",
    "        if len(block_as_str) == 1:\n",
    "            block_as_str = \"00\" + block_as_str\n",
    "        else:\n",
    "            block_as_str = \"0\" + block_as_str\n",
    "            \n",
    "        processed_fragment =  create_padded_buckets(seq, bucket_info=bucket_info, has_target=has_target, \n",
    "                                                    save_to_file_path=os.path.join(save_to_path, \n",
    "                                                                                   f\"processed_chunk_{block_as_str}.pkl\"))\n",
    "        block += 1"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:47:54.650972Z",
     "iopub.execute_input": "2025-04-18T11:47:54.651250Z",
     "iopub.status.idle": "2025-04-18T11:47:54.682076Z",
     "shell.execute_reply.started": "2025-04-18T11:47:54.651230Z",
     "shell.execute_reply": "2025-04-18T11:47:54.681234Z"
    }
   },
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Разобьем обучающие данные на тренировочную и валидационную выборки. Воспользуемся самым простым способом &ndash; для валидации случайным образом выберем 10% обучающих данных."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "train, val = train_test_split(train_target, random_state=42, test_size=0.1)\n",
    "train.shape, val.shape"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:48:00.058710Z",
     "iopub.execute_input": "2025-04-18T11:48:00.059012Z",
     "iopub.status.idle": "2025-04-18T11:48:00.272063Z",
     "shell.execute_reply.started": "2025-04-18T11:48:00.058991Z",
     "shell.execute_reply": "2025-04-18T11:48:00.271154Z"
    }
   },
   "outputs": [
    {
     "execution_count": 11,
     "output_type": "execute_result",
     "data": {
      "text/plain": "((2700000, 2), (300000, 2))"
     },
     "metadata": {}
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "TRAIN_BUCKETS_PATH = \"data/train_buckets_rnn\"\n",
    "VAL_BUCKETS_PATH = \"data/val_buckets_rnn\"\n",
    "TEST_BUCKETS_PATH = \"data/test_buckets_rnn\""
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T11:48:01.980920Z",
     "iopub.execute_input": "2025-04-18T11:48:01.981233Z",
     "iopub.status.idle": "2025-04-18T11:48:02.007651Z",
     "shell.execute_reply.started": "2025-04-18T11:48:01.981207Z",
     "shell.execute_reply": "2025-04-18T11:48:02.007026Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "cell_type": "code",
   "source": [
    "import shutil\n",
    "\n",
    "for buckets_path in [TRAIN_BUCKETS_PATH, VAL_BUCKETS_PATH, TEST_BUCKETS_PATH]:\n",
    "    if os.path.exists(buckets_path):\n",
    "        print(f\"Удаляю папку {buckets_path}\")\n",
    "        shutil.rmtree(buckets_path)  # Удаляем директорию и её содержимое\n",
    "    print(f\"Создаю папку {buckets_path}\")\n",
    "    os.makedirs(buckets_path)  # Создаём директорию"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:00:47.801154Z",
     "iopub.execute_input": "2025-04-18T12:00:47.801522Z",
     "iopub.status.idle": "2025-04-18T12:00:47.847172Z",
     "shell.execute_reply.started": "2025-04-18T12:00:47.801491Z",
     "shell.execute_reply": "2025-04-18T12:00:47.846380Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Создаю папку data/train_buckets_rnn\nСоздаю папку data/val_buckets_rnn\nСоздаю папку data/test_buckets_rnn\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "create_buckets_from_credits(TRAIN_DATA_PATH,\n",
    "                            bucket_info=bucket_info,\n",
    "                            save_to_path=TRAIN_BUCKETS_PATH,\n",
    "                            frame_with_ids=train,\n",
    "                            num_parts_to_preprocess_at_once=3, \n",
    "                            num_parts_total=12, has_target=True)\n",
    "\n",
    "dataset_train = sorted([os.path.join(TRAIN_BUCKETS_PATH, x) for x in os.listdir(TRAIN_BUCKETS_PATH)])\n",
    "dataset_train"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:00:51.568759Z",
     "iopub.execute_input": "2025-04-18T12:00:51.569042Z",
     "iopub.status.idle": "2025-04-18T12:11:07.969864Z",
     "shell.execute_reply.started": "2025-04-18T12:00:51.569021Z",
     "shell.execute_reply": "2025-04-18T12:11:07.969103Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Preparing credit data:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68ac5820d78242b5b1635295b5d90591"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_0.pq\n/kaggle/input/credit/data/train_data/train_data_1.pq\n/kaggle/input/credit/data/train_data/train_data_2.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "71ac3d8e01cb4f3a80fe9703e2c02f58"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/43 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "94fe6627a8754339aa18e3e20874aefa"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_3.pq\n/kaggle/input/credit/data/train_data/train_data_4.pq\n/kaggle/input/credit/data/train_data/train_data_5.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "68ad3283590f4866a842ffe2c43829b4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/42 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a8ef140b132b48ffb3bbea2f73460f50"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_6.pq\n/kaggle/input/credit/data/train_data/train_data_7.pq\n/kaggle/input/credit/data/train_data/train_data_8.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f455989a90234132903c8d50e3266cc4"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/43 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a27a25a60e7847dabc148526ab50bdb1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_9.pq\n/kaggle/input/credit/data/train_data/train_data_10.pq\n/kaggle/input/credit/data/train_data/train_data_11.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8877fd863ef54ccfb6fe5c9d94f396b5"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/42 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "23ecc2b2453e4b2b8931fb5cde215599"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "CPU times: user 9min 24s, sys: 1min 13s, total: 10min 38s\nWall time: 10min 16s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 17,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['data/train_buckets_rnn/processed_chunk_000.pkl',\n 'data/train_buckets_rnn/processed_chunk_001.pkl',\n 'data/train_buckets_rnn/processed_chunk_002.pkl',\n 'data/train_buckets_rnn/processed_chunk_003.pkl']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "create_buckets_from_credits(TRAIN_DATA_PATH,\n",
    "                            bucket_info=bucket_info,\n",
    "                            save_to_path=VAL_BUCKETS_PATH,\n",
    "                            frame_with_ids=val,\n",
    "                            num_parts_to_preprocess_at_once=3, \n",
    "                            num_parts_total=12, has_target=True)\n",
    "\n",
    "dataset_val = sorted([os.path.join(VAL_BUCKETS_PATH, x) for x in os.listdir(VAL_BUCKETS_PATH)])\n",
    "dataset_val"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:11:07.971065Z",
     "iopub.execute_input": "2025-04-18T12:11:07.971367Z",
     "iopub.status.idle": "2025-04-18T12:20:19.741983Z",
     "shell.execute_reply.started": "2025-04-18T12:11:07.971335Z",
     "shell.execute_reply": "2025-04-18T12:20:19.741068Z"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Preparing credit data:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "928ff0d7d8814a84981039b1cd9fe2da"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_0.pq\n/kaggle/input/credit/data/train_data/train_data_1.pq\n/kaggle/input/credit/data/train_data/train_data_2.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8e15568040e44a65a36bb4f801e87b83"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db76f167b93340478ec3f4da67323209"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_3.pq\n/kaggle/input/credit/data/train_data/train_data_4.pq\n/kaggle/input/credit/data/train_data/train_data_5.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b2984c5845a4f02a219424fe5296034"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fe42cb454ddd4093907a1cec89f2a5c1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_6.pq\n/kaggle/input/credit/data/train_data/train_data_7.pq\n/kaggle/input/credit/data/train_data/train_data_8.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b83f6bcad854b849539926803bdc4aa"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/41 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7876d957a39741ca8ad583c2a08c76ee"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Reading chunks:\n/kaggle/input/credit/data/train_data/train_data_9.pq\n/kaggle/input/credit/data/train_data/train_data_10.pq\n/kaggle/input/credit/data/train_data/train_data_11.pq\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Reading dataset with pandas:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "caaa57460253412cbfcef3888f9290ae"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Transforming credits to sequences is done.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting buckets:   0%|          | 0/43 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6104bc028ae84b67b2298d0df6669291"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "CPU times: user 8min 58s, sys: 46.1 s, total: 9min 44s\nWall time: 9min 11s\n",
     "output_type": "stream"
    },
    {
     "execution_count": 18,
     "output_type": "execute_result",
     "data": {
      "text/plain": "['data/val_buckets_rnn/processed_chunk_000.pkl',\n 'data/val_buckets_rnn/processed_chunk_001.pkl',\n 'data/val_buckets_rnn/processed_chunk_002.pkl',\n 'data/val_buckets_rnn/processed_chunk_003.pkl']"
     },
     "metadata": {}
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Modeling"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cpu')\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "print('Using device:', device)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:20:19.743296Z",
     "iopub.execute_input": "2025-04-18T12:20:19.743542Z",
     "iopub.status.idle": "2025-04-18T12:20:19.778062Z",
     "shell.execute_reply.started": "2025-04-18T12:20:19.743521Z",
     "shell.execute_reply": "2025-04-18T12:20:19.777262Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "Using device: cuda\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Для создания модели будем использовать фреймворк `torch`. В нем есть все, чтобы писать произвольные сложные архитектуры и быстро экспериментировать. Для того, чтобы мониторить и логировать весь процесс во время обучения сетей, рекомендуется использовать надстройки над данным фреймворком, например, `lightning`.\n",
    "\n",
    "* В бейзлайне мы предлагаем базовые компоненты, чтобы можно было обучать нейронную сеть и отслеживать ее качество. Для этого вам предоставлены следующие функции:\n",
    "    * `data_generators.batches_generator` &ndash; функция-генератор, итеративно возвращает батчи, поддерживает батчи для `tensorflow.keras` и `torch.nn.Module` моделей. В зависимости от флага `is_train` может быть использована для генерации батчей на train/val/test стадии.\n",
    "    * функция `pytorch_training.train_epoch` &ndash; обучает модель одну эпоху.\n",
    "    * функция `pytorch_training.eval_model` &ndash; проверяет качество модели на отложенной выборке и возвращает roc_auc_score.\n",
    "    * функция `pytorch_training.inference` &ndash; делает предикты на новых данных и готовит фрейм для проверяющей системы.\n",
    "    * класс `training_aux.EarlyStopping` &ndash; реализует early_stopping, сохраняя лучшую модель. Пример использования приведен ниже."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from data_generators import batches_generator\n",
    "from pytorch_training import train_epoch, eval_model, inference\n",
    "from training_aux import EarlyStopping"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:21:51.594408Z",
     "iopub.execute_input": "2025-04-18T12:21:51.594797Z",
     "iopub.status.idle": "2025-04-18T12:21:51.638849Z",
     "shell.execute_reply.started": "2025-04-18T12:21:51.594763Z",
     "shell.execute_reply": "2025-04-18T12:21:51.638130Z"
    }
   },
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Все признаки, описывающие кредитную историю клиентов &ndash; категориальные. Для их представления в модели используем категориальные эмбеддинги. Для этого нужно каждому категориальному признаку задать размерность латентного пространства. Используем [формулу](https://forums.fast.ai/t/size-of-embedding-for-categorical-variables/42608) из библиотеки `fast.ai`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_embed_dim(n_cat: int) -> int:\n",
    "    return min(600, round(1.6 * n_cat**0.56))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:21:56.961167Z",
     "iopub.execute_input": "2025-04-18T12:21:56.961467Z",
     "iopub.status.idle": "2025-04-18T12:21:56.988881Z",
     "shell.execute_reply.started": "2025-04-18T12:21:56.961426Z",
     "shell.execute_reply": "2025-04-18T12:21:56.988113Z"
    }
   },
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_projections = {feat: (max(uniq)+1, compute_embed_dim(max(uniq)+1)) for feat, uniq in uniques.items()}\n",
    "embedding_projections"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T12:23:30.900834Z",
     "iopub.execute_input": "2025-04-18T12:23:30.901181Z",
     "iopub.status.idle": "2025-04-18T12:23:30.935499Z",
     "shell.execute_reply.started": "2025-04-18T12:23:30.901147Z",
     "shell.execute_reply": "2025-04-18T12:23:30.934594Z"
    }
   },
   "outputs": [
    {
     "execution_count": 24,
     "output_type": "execute_result",
     "data": {
      "text/plain": "{'pre_since_opened': (20, 9),\n 'pre_since_confirmed': (18, 8),\n 'pre_pterm': (18, 8),\n 'pre_fterm': (17, 8),\n 'pre_till_pclose': (17, 8),\n 'pre_till_fclose': (16, 8),\n 'pre_loans_credit_limit': (20, 9),\n 'pre_loans_next_pay_summ': (8, 5),\n 'pre_loans_outstanding': (6, 4),\n 'pre_loans_total_overdue': (2, 2),\n 'pre_loans_max_overdue_sum': (4, 3),\n 'pre_loans_credit_cost_rate': (14, 7),\n 'pre_loans5': (18, 8),\n 'pre_loans530': (20, 9),\n 'pre_loans3060': (10, 6),\n 'pre_loans6090': (6, 4),\n 'pre_loans90': (20, 9),\n 'is_zero_loans5': (2, 2),\n 'is_zero_loans530': (2, 2),\n 'is_zero_loans3060': (2, 2),\n 'is_zero_loans6090': (2, 2),\n 'is_zero_loans90': (2, 2),\n 'pre_util': (20, 9),\n 'pre_over2limit': (20, 9),\n 'pre_maxover2limit': (20, 9),\n 'is_zero_util': (2, 2),\n 'is_zero_over2limit': (2, 2),\n 'is_zero_maxover2limit': (2, 2),\n 'enc_paym_0': (4, 3),\n 'enc_paym_1': (4, 3),\n 'enc_paym_2': (4, 3),\n 'enc_paym_3': (4, 3),\n 'enc_paym_4': (4, 3),\n 'enc_paym_5': (4, 3),\n 'enc_paym_6': (4, 3),\n 'enc_paym_7': (4, 3),\n 'enc_paym_8': (4, 3),\n 'enc_paym_9': (4, 3),\n 'enc_paym_10': (4, 3),\n 'enc_paym_11': (5, 4),\n 'enc_paym_12': (4, 3),\n 'enc_paym_13': (4, 3),\n 'enc_paym_14': (4, 3),\n 'enc_paym_15': (4, 3),\n 'enc_paym_16': (4, 3),\n 'enc_paym_17': (4, 3),\n 'enc_paym_18': (4, 3),\n 'enc_paym_19': (4, 3),\n 'enc_paym_20': (5, 4),\n 'enc_paym_21': (4, 3),\n 'enc_paym_22': (4, 3),\n 'enc_paym_23': (4, 3),\n 'enc_paym_24': (5, 4),\n 'enc_loans_account_holder_type': (7, 5),\n 'enc_loans_credit_status': (7, 5),\n 'enc_loans_credit_type': (8, 5),\n 'enc_loans_account_cur': (4, 3),\n 'pclose_flag': (2, 2),\n 'fclose_flag': (2, 2)}"
     },
     "metadata": {}
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Реализуем модель. Все входные признаки представим в виде эмбеддингов, сконкатенируем, чтобы получить векторное представление транзакции. Подадим последовательности в `GRU` рекуррентный слой. Используем последнее скрытое состояние в качестве выхода слоя. На основе такого входа построим небольшой `MLP`, выступающий классификатором для целевой задачи. Используем градиентный спуск, чтобы решить оптимизационную задачу."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "class RNN_GRU(nn.Module):\n",
    "    def __init__(self,\n",
    "                 features,\n",
    "                 embedding_projections,\n",
    "                 rnn_units=128,\n",
    "                 top_classifier_units=32,\n",
    "                 bidirectional=False,\n",
    "                 num_layers=1,\n",
    "                 spatial_dropout = 0.0\n",
    "                ):\n",
    "        super(RNN_GRU, self).__init__()\n",
    "       \n",
    "        self._credits_cat_embeddings = nn.ModuleList([\n",
    "            self._create_embedding_projection(*embedding_projections[feature])\n",
    "            for feature in features\n",
    "        ])\n",
    "        \n",
    "        input_size = sum([embedding_projections[x][1] for x in features])\n",
    "        self._spatial_dropout = nn.Dropout1d(spatial_dropout)\n",
    "        self._gru = nn.GRU(input_size=input_size,\n",
    "                            hidden_size=rnn_units,\n",
    "                            batch_first=True,\n",
    "                            bidirectional=bidirectional,\n",
    "                            num_layers=num_layers,\n",
    "                            )\n",
    "        \n",
    "        self._hidden_size = rnn_units * (2 if bidirectional else 1)\n",
    "\n",
    "        self._maxpool = nn.AdaptiveMaxPool1d(1)\n",
    "        self._avgpool = nn.AdaptiveAvgPool1d(1)\n",
    "        self._top_classifier = nn.Sequential(nn.Linear(in_features=self._hidden_size * 2, \n",
    "                                                       out_features=top_classifier_units),\n",
    "                                             nn.ReLU(),\n",
    "                                             nn.Linear(in_features=top_classifier_units, out_features=1)\n",
    "                                            )\n",
    "        \n",
    "        \n",
    "    def forward(self, features):\n",
    "        batch_size = features[0].shape[0]\n",
    "        embeddings = [embedding(features[i]) for i, embedding in enumerate(self._credits_cat_embeddings)]\n",
    "        concated_embeddings = torch.cat(embeddings, dim=-1)  # [B, T, input_size]\n",
    "        embeddings_permute = concated_embeddings.permute(0, 2, 1)\n",
    "        embeddings_dropped = self._spatial_dropout(embeddings_permute)\n",
    "        embeddings = embeddings_dropped.permute(0, 2, 1)  # [B, T, input_size]\n",
    "        \n",
    "        \n",
    "        gru_out, _ = self._gru(embeddings)  # [B, T, hidden_size]\n",
    "        \n",
    "        gp_input = gru_out.permute(0, 2, 1) # [B, input_size, T]\n",
    "        max_pooled = self._maxpool(gp_input).squeeze(-1)  # [B, hidden_size]\n",
    "        avg_pooled = self._avgpool(gp_input).squeeze(-1)  # [B, hidden_size]\n",
    "\n",
    "        hidden = torch.cat([max_pooled, avg_pooled], dim=1)  # [B, hidden_size * 2]\n",
    "        \n",
    "        logit = self._top_classifier(hidden)\n",
    "        return logit\n",
    "        \n",
    "    @classmethod\n",
    "    def _create_embedding_projection(cls, cardinality, embed_size, add_missing=True, padding_idx=0):\n",
    "        add_missing = 1 if add_missing else 0\n",
    "        return nn.Embedding(num_embeddings=cardinality + add_missing, embedding_dim=embed_size, padding_idx=padding_idx)\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:12:59.279972Z",
     "iopub.execute_input": "2025-04-18T13:12:59.280351Z",
     "iopub.status.idle": "2025-04-18T13:12:59.324058Z",
     "shell.execute_reply.started": "2025-04-18T13:12:59.280321Z",
     "shell.execute_reply": "2025-04-18T13:12:59.323129Z"
    }
   },
   "outputs": [],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "shutil.rmtree('./checkpoints/', ignore_errors=True)\n",
    "os.makedirs('./checkpoints/', exist_ok=True)"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:03.928187Z",
     "iopub.execute_input": "2025-04-18T13:13:03.928487Z",
     "iopub.status.idle": "2025-04-18T13:13:03.963483Z",
     "shell.execute_reply.started": "2025-04-18T13:13:03.928462Z",
     "shell.execute_reply": "2025-04-18T13:13:03.962683Z"
    }
   },
   "outputs": [],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Запустим цикл обучения, каждую эпоху будем логировать лосс, а так же ROC-AUC на валидации и на обучении. Будем сохрнаять веса после каждой эпохи, а так же лучшие с помощью early_stopping."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def train_model(model,\n",
    "                optimizer,\n",
    "                dataset_train,\n",
    "                dataset_val,\n",
    "                num_epochs, \n",
    "                train_batch_size,\n",
    "                val_batch_size,\n",
    "                path_to_checkpoints,\n",
    "                es,\n",
    "                device,\n",
    "                scheduler=None):\n",
    "    \"\"\"\n",
    "    Обучает модель, отслеживая значения ROC AUC для тренировки и валидации.\n",
    "\n",
    "    Возвращает:\n",
    "      best_model_path (str): путь к лучшей модели по валидационному ROC AUC.\n",
    "      best_roc_auc (float): наилучшее значение ROC AUC на валидационной выборке.\n",
    "      train_roc_auc_list (list): список ROC AUC на тренировке по эпохам.\n",
    "      val_roc_auc_list (list): список ROC AUC на валидации по эпохам.\n",
    "    \"\"\"\n",
    "    best_roc_auc = -float(\"inf\")\n",
    "    best_model_path = None\n",
    "    train_roc_auc_list = []\n",
    "    val_roc_auc_list = []\n",
    "    \n",
    "    for epoch in trange(num_epochs, desc=\"Training Epochs\", unit=\"epoch\", leave=True):\n",
    "        # Обучаем модель на тренировочном датасете\n",
    "        train_epoch(model, optimizer, dataset_train, batch_size=train_batch_size, \n",
    "                    shuffle=True, print_loss_every_n_batches=500, device=device)\n",
    "        \n",
    "        # Оцениваем модель на валидационном датасете\n",
    "        current_val_roc_auc = eval_model(model, dataset_val, batch_size=val_batch_size, device=device)\n",
    "        val_roc_auc_list.append(current_val_roc_auc)\n",
    "        \n",
    "        # Оцениваем модель на тренировочном датасете (для контроля переобучения)\n",
    "        current_train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_size, device=device)\n",
    "        train_roc_auc_list.append(current_train_roc_auc)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1} completed. Train ROC AUC: {current_train_roc_auc}, Val ROC AUC: {current_val_roc_auc}\")\n",
    "       \n",
    "        es(current_val_roc_auc, model)\n",
    "        if es.early_stop:\n",
    "            print(\"Early stopping reached. Stop training...\")\n",
    "            break\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        # Сохраняем чекпоинт текущей эпохи (для истории)\n",
    "        checkpoint_path = os.path.join(path_to_checkpoints, f\"epoch_{epoch+1}_val_{current_val_roc_auc:.3f}.pt\")\n",
    "        torch.save(model.state_dict(), checkpoint_path)\n",
    "    best_roc_auc = max(val_roc_auc_list)\n",
    "    print(f\"Training finished. Best model saved at: {best_model_path} with Val ROC AUC: {best_roc_auc}\")\n",
    "    return best_model_path, best_roc_auc, train_roc_auc_list, val_roc_auc_list\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:06.679100Z",
     "iopub.execute_input": "2025-04-18T13:13:06.679406Z",
     "iopub.status.idle": "2025-04-18T13:13:06.716658Z",
     "shell.execute_reply.started": "2025-04-18T13:13:06.679380Z",
     "shell.execute_reply": "2025-04-18T13:13:06.715839Z"
    }
   },
   "outputs": [],
   "execution_count": 48
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_configs = [\n",
    "{\n",
    "        \"name\": \"exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1\",\n",
    "        \"model_name\": RNN_GRU,  \n",
    "        \"optimizer\": \"adam\",\n",
    "        \"optimizer_params\": {\n",
    "             \"lr\": 0.001,\n",
    "            },\n",
    "        \"num_epochs\": 20,\n",
    "        \"train_batch_size\": 128,\n",
    "        \"val_batch_size\": 128,\n",
    "        \"model_params\": {\n",
    "            \"features\":features,\n",
    "            \"embedding_projections\": embedding_projections,\n",
    "            \"rnn_units\": 128,\n",
    "            \"top_classifier_units\": 32,\n",
    "            \"bidirectional\": True,\n",
    "            \"num_layers\": 1,\n",
    "            \"spatial_dropout\":0.05\n",
    "            },\n",
    "        \"schedule\":True,\n",
    "        \"early_stopping_patience\": 3\n",
    "    }\n",
    "]"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:26.824197Z",
     "iopub.execute_input": "2025-04-18T13:13:26.824496Z",
     "iopub.status.idle": "2025-04-18T13:13:26.859056Z",
     "shell.execute_reply.started": "2025-04-18T13:13:26.824473Z",
     "shell.execute_reply": "2025-04-18T13:13:26.858350Z"
    }
   },
   "outputs": [],
   "execution_count": 49
  },
  {
   "cell_type": "code",
   "source": [
    "print(len(experiment_configs))"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:27.678906Z",
     "iopub.execute_input": "2025-04-18T13:13:27.679219Z",
     "iopub.status.idle": "2025-04-18T13:13:27.714005Z",
     "shell.execute_reply.started": "2025-04-18T13:13:27.679193Z",
     "shell.execute_reply": "2025-04-18T13:13:27.713004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": "1\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.optim.lr_scheduler import StepLR"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:29.519318Z",
     "iopub.execute_input": "2025-04-18T13:13:29.519632Z",
     "iopub.status.idle": "2025-04-18T13:13:29.554088Z",
     "shell.execute_reply.started": "2025-04-18T13:13:29.519604Z",
     "shell.execute_reply": "2025-04-18T13:13:29.553220Z"
    }
   },
   "outputs": [],
   "execution_count": 51
  },
  {
   "cell_type": "code",
   "source": [
    "experiment_results = []\n",
    "\n",
    "for config in tqdm.tqdm(experiment_configs, desc=\"Experiments\", unit=\"experiment\"):\n",
    "    print(f\"\\n{'='*40}\\nStarting experiment: {config['name']}\")\n",
    "    \n",
    "    checkpoint_dir = os.path.join(\"checkpoints\", config[\"name\"])\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Создание модели через передачу класса\n",
    "    model = config[\"model_name\"](**config[\"model_params\"])\n",
    "    model.to(device)\n",
    "    \n",
    "    # Настройка оптимизатора\n",
    "    optimizer_name = config[\"optimizer\"].lower()\n",
    "    if optimizer_name == \"adam\":\n",
    "        optimizer = torch.optim.Adam(model.parameters(),**config[\"optimizer_params\"])\n",
    "    elif optimizer_name == \"sgd\":\n",
    "        optimizer = torch.optim.SGD(model.parameters(),**config[\"optimizer_params\"])\n",
    "    elif optimizer_name == 'adamw':\n",
    "        optimizer = torch.optim.AdamW(model.parameters(),**config[\"optimizer_params\"])\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный оптимизатор: {config['optimizer']}\")\n",
    "    \n",
    "\n",
    "    es = EarlyStopping(patience=config.get(\"early_stopping_patience\", 3),\n",
    "                       verbose=True,\n",
    "                       mode='max',\n",
    "                       save_path=os.path.join(checkpoint_dir, \"best_checkpoint.pt\"),\n",
    "                       metric_name=\"ROC-AUC\",\n",
    "                       save_format=\"torch\",\n",
    "                      )\n",
    "    scheduler = None\n",
    "    if config[\"schedule\"]:\n",
    "        scheduler = StepLR(optimizer, step_size=3, gamma=0.5)        \n",
    "        \n",
    "    # Запуск обучения\n",
    "    best_model_path, best_roc_auc, train_roc_auc_list, val_roc_auc_list = train_model(\n",
    "        model=model,\n",
    "        optimizer=optimizer,\n",
    "        dataset_train=dataset_train,\n",
    "        dataset_val=dataset_val,\n",
    "        num_epochs=config[\"num_epochs\"],\n",
    "        train_batch_size=config[\"train_batch_size\"],\n",
    "        val_batch_size=config[\"val_batch_size\"],\n",
    "        path_to_checkpoints=checkpoint_dir,\n",
    "        es=es,\n",
    "        device=device,\n",
    "        scheduler = scheduler\n",
    "    )\n",
    "    \n",
    "    print(f\"Experiment {config['name']} finished. Best model saved at: {os.path.join(checkpoint_dir, 'best_checkpoint.pt')}\")\n",
    "    \n",
    "    experiment_results.append({\n",
    "        \"experiment\": config[\"name\"],\n",
    "        \"model_name\": config[\"model_name\"].__name__,  # Получаем имя класса модели\n",
    "        \"optimizer\": config[\"optimizer\"],\n",
    "        \"optimizer_params\": config[\"optimizer_params\"],\n",
    "        \"num_epochs\": config[\"num_epochs\"],\n",
    "        \"train_batch_size\": config[\"train_batch_size\"],\n",
    "        \"val_batch_size\": config[\"val_batch_size\"],\n",
    "        \"model_params\": config[\"model_params\"],\n",
    "        \"early_stopping_patience\": config.get(\"early_stopping_patience\", 3),\n",
    "        \"best_model_path\": best_model_path,\n",
    "        \"best_roc_auc\": best_roc_auc,\n",
    "        \"train_roc_auc\": train_roc_auc_list,\n",
    "        \"val_roc_auc\": val_roc_auc_list,\n",
    "    })\n",
    "\n"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:13:31.175268Z",
     "iopub.execute_input": "2025-04-18T13:13:31.175573Z",
     "iopub.status.idle": "2025-04-18T14:09:35.163816Z",
     "shell.execute_reply.started": "2025-04-18T13:13:31.175546Z",
     "shell.execute_reply": "2025-04-18T14:09:35.160683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "text": "Experiments:   0%|          | 0/1 [00:00<?, ?experiment/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "\n========================================\nStarting experiment: exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:   0%|          | 0/20 [00:00<?, ?epoch/s]\u001B[A",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f66e0a2ecbe04f0abf204f66900849cc"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13715508580207825\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b6dc05f8f19046b399c9a801706bd729"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1f361cb4e92948cb981174b4a7027d96"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:   5%|▌         | 1/20 [07:06<2:14:56, 426.12s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1 completed. Train ROC AUC: 0.7783136406911497, Val ROC AUC: 0.7732905725692046\nValidation ROC-AUC improved (-inf --> 0.773291).  Saving model...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e71bd39436324e568eeac7c13aaa4fe9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13466686010360718\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f339a386323746daaeecd22b4206034d"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "019a8515ba284951b13b0fb6f86a6044"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  10%|█         | 2/20 [14:23<2:09:47, 432.64s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 2 completed. Train ROC AUC: 0.7854080116897034, Val ROC AUC: 0.7784214564685675\nValidation ROC-AUC improved (0.773291 --> 0.778421).  Saving model...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0882ebea955d4528af2459668dd6094c"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13365010917186737\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3510b29a0d54b1a96f2cfeca810fc02"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "58b5513c0cc04c64a0d80f2c24bd1b00"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  15%|█▌        | 3/20 [21:12<1:59:33, 421.94s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 3 completed. Train ROC AUC: 0.7839515161605166, Val ROC AUC: 0.7740088225787389\nNo imporvement in validation ROC-AUC. Current: 0.774009. Current best: 0.778421\nEarlyStopping counter: 1 out of 3\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8b6fa907ac144434ad7c8a93afcbd596"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13247673213481903\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c04e102134be4b9ba2273e5dd84280f2"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8a5611501cf2424f89eddf75df1f7a46"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  20%|██        | 4/20 [28:05<1:51:32, 418.26s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 4 completed. Train ROC AUC: 0.7957414389757285, Val ROC AUC: 0.7794941254177873\nValidation ROC-AUC improved (0.778421 --> 0.779494).  Saving model...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fdbe75ea1b7e482a971ac302b2b34ef9"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13161925971508026\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d8ffd450493442ea2706f77183c5ea6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a3789537553e40bd84cc319edbe304ea"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  25%|██▌       | 5/20 [35:10<1:45:10, 420.70s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 5 completed. Train ROC AUC: 0.8021981412020439, Val ROC AUC: 0.781063420522984\nValidation ROC-AUC improved (0.779494 --> 0.781063).  Saving model...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c83e28179792444a86d41765e1f54db1"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.13089700043201447\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "670535aaa65141508eaa88732ffaf90b"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "daf87fc283a14597a9c928b6025fed0f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  30%|███       | 6/20 [42:08<1:37:56, 419.77s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 6 completed. Train ROC AUC: 0.8069527036622512, Val ROC AUC: 0.7803284032862693\nNo imporvement in validation ROC-AUC. Current: 0.780328. Current best: 0.781063\nEarlyStopping counter: 1 out of 3\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d045b48fdf20403f9d2720ff72573b6a"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.12946833670139313\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8985aaa49fbf4a28ba86238c0f48737a"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4940e08e1d024cf59ed57f0aa305cd39"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "\nTraining Epochs:  35%|███▌      | 7/20 [49:07<1:30:53, 419.48s/epoch]\u001B[A",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 7 completed. Train ROC AUC: 0.8140643434560768, Val ROC AUC: 0.779906578128664\nNo imporvement in validation ROC-AUC. Current: 0.779907. Current best: 0.781063\nEarlyStopping counter: 2 out of 3\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Training: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "753e01f571cf4a49aee51ff1e27acb0f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Training loss after epoch: 0.12878145277500153\r",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "24f9cf17b4014dcba50d140c4e8f3518"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Evaluating model: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9a786640b85f46d0846fb7324fea9fc8"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Training Epochs:  35%|███▌      | 7/20 [56:03<1:44:07, 480.56s/epoch]\nExperiments: 100%|██████████| 1/1 [56:03<00:00, 3363.94s/experiment]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 8 completed. Train ROC AUC: 0.8162742585674971, Val ROC AUC: 0.775620612950704\nNo imporvement in validation ROC-AUC. Current: 0.775621. Current best: 0.781063\nEarlyStopping counter: 3 out of 3\nEarly stopping reached. Stop training...\nTraining finished. Best model saved at: None with Val ROC AUC: 0.781063420522984\nExperiment exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1 finished. Best model saved at: checkpoints/exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1/best_checkpoint.pt\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ],
   "execution_count": 52
  },
  {
   "cell_type": "code",
   "source": [
    "# --- Сохранение результатов в DataFrame ---\n",
    "df_results = pd.DataFrame(experiment_results)\n",
    "df_results"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-18T13:04:40.570164Z",
     "iopub.execute_input": "2025-04-18T13:04:40.570783Z",
     "iopub.status.idle": "2025-04-18T13:04:40.707807Z",
     "shell.execute_reply.started": "2025-04-18T13:04:40.570753Z",
     "shell.execute_reply": "2025-04-18T13:04:40.706989Z"
    }
   },
   "outputs": [
    {
     "execution_count": 33,
     "output_type": "execute_result",
     "data": {
      "text/plain": "                                        experiment model_name optimizer  \\\n0  exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1    RNN_GRU      adam   \n\n  optimizer_params  num_epochs  train_batch_size  val_batch_size  \\\n0    {'lr': 0.001}          20               128             128   \n\n                                        model_params  early_stopping_patience  \\\n0  {'features': ['pre_since_opened', 'pre_since_c...                        3   \n\n  best_model_path  best_roc_auc  \\\n0            None          -inf   \n\n                                       train_roc_auc  \\\n0  [0.7753407063024896, 0.7839057216889576, 0.788...   \n\n                                         val_roc_auc  \n0  [0.7699765674259503, 0.7761341606764023, 0.778...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>experiment</th>\n      <th>model_name</th>\n      <th>optimizer</th>\n      <th>optimizer_params</th>\n      <th>num_epochs</th>\n      <th>train_batch_size</th>\n      <th>val_batch_size</th>\n      <th>model_params</th>\n      <th>early_stopping_patience</th>\n      <th>best_model_path</th>\n      <th>best_roc_auc</th>\n      <th>train_roc_auc</th>\n      <th>val_roc_auc</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>exp_GRU_adam_lr_001_sch3_rnn_units128_tpu32_bd1</td>\n      <td>RNN_GRU</td>\n      <td>adam</td>\n      <td>{'lr': 0.001}</td>\n      <td>20</td>\n      <td>128</td>\n      <td>128</td>\n      <td>{'features': ['pre_since_opened', 'pre_since_c...</td>\n      <td>3</td>\n      <td>None</td>\n      <td>-inf</td>\n      <td>[0.7753407063024896, 0.7839057216889576, 0.788...</td>\n      <td>[0.7699765674259503, 0.7761341606764023, 0.778...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {}
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "code",
   "source": [
    "df.to_csv('experiments_relults.csv', index=False, encoding='utf-8')"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-16T17:51:36.519609Z",
     "iopub.status.idle": "2025-04-16T17:51:36.519841Z",
     "shell.execute_reply": "2025-04-16T17:51:36.519745Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "%%time\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Starting epoch {epoch+1}\")\n",
    "    train_epoch(model, optimizer, dataset_train, batch_size=train_batch_size, \n",
    "                shuffle=True, print_loss_every_n_batches=500, device=device)\n",
    "    \n",
    "    val_roc_auc = eval_model(model, dataset_val, batch_size=val_batch_size, device=device)\n",
    "    es(val_roc_auc, model)\n",
    "    \n",
    "    if es.early_stop:\n",
    "        print(\"Early stopping reached. Stop training...\")\n",
    "        break\n",
    "    torch.save(model.state_dict(), os.path.join(path_to_checkpoints, f\"epoch_{epoch+1}_val_{val_roc_auc:.3f}.pt\"))\n",
    "    \n",
    "    train_roc_auc = eval_model(model, dataset_train, batch_size=val_batch_size, device=device)\n",
    "    print(f\"Epoch {epoch+1} completed. Train ROC AUC: {train_roc_auc}, val ROC AUC: {val_roc_auc}\")"
   ],
   "metadata": {
    "trusted": true,
    "execution": {
     "iopub.status.busy": "2025-04-16T17:34:15.417548Z",
     "iopub.status.idle": "2025-04-16T17:34:15.417858Z",
     "shell.execute_reply": "2025-04-16T17:34:15.417744Z"
    }
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
